{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spdsp04/Gesture_Recognition/blob/main/Gesture_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgageSzSAAPY"
      },
      "source": [
        "# Gesture Recognition\n",
        "![1607_2716_0 (1)](https://user-images.githubusercontent.com/93203186/172011187-70099885-05f6-4e4a-9dbe-e4db30471f54.jpg)\n",
        "\n",
        "\n",
        "> Imagine working as a data scientist at a home electronics company which manufactures state of the art smart televisions. You want to develop a cool feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote. \n",
        "> The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n",
        "\n",
        "- Thumbs up:  Increase the volume\n",
        "- Thumbs down: Decrease the volume\n",
        "- Left swipe: 'Jump' backwards 10 seconds\n",
        "- Right swipe: 'Jump' forward 10 seconds  \n",
        "- Stop: Pause the movie\n",
        "\n",
        "## Understanding the Dataset\n",
        "> The training data consists of a few hundred videos categorised into one of the five classes. Each video (typically 2-3 seconds long) is divided into a sequence of 30 frames(images). These videos have been recorded by various people performing one of the five gestures in front of a webcam - similar to what the smart TV will use. \n",
        "\n",
        "> The data is in a zip file. The zip file contains a 'train' and a 'val' folder with two CSV files for the two folders. These folders are in turn divided into subfolders where each subfolder represents a video of a particular gesture. Each subfolder, i.e. a video, contains 30 frames (or images). Note that all images in a particular video subfolder have the same dimensions but different videos may have different dimensions. Specifically, videos have two types of dimensions - either 360x360 or 120x160 (depending on the webcam used to record the videos). Hence, you will need to do some pre-processing to standardise the videos. \n",
        "\n",
        "> Each row of the CSV file represents one video and contains three main pieces of information - the name of the subfolder containing the 30 images of the video, the name of the gesture and the numeric label (between 0-4) of the video.\n",
        "\n",
        "> Here task is to train a model on the 'train' folder which performs well on the 'val' folder as well (as usually done in ML projects). We have withheld the test folder for evaluation purposes - your final model's performance will be tested on the 'test' set.\n",
        " \n",
        " ## Goals of this Project\n",
        "\n",
        "> __Generator:__  The generator should be able to take a batch of videos as input without any error. Steps like cropping, resizing and normalization should be performed successfully.\n",
        "\n",
        "> __Model:__ Develop a model that is able to train without any errors which will be judged on the total number of parameters (as the inference(prediction) time should be less) and the accuracy achieved.\n",
        "\n",
        "> __Write up:__ This should contain the detailed procedure followed in choosing the final model. The write up should start with the reason for choosing the base model, then highlight the reasons and metrics taken into consideration to modify and experiment to arrive at the final model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HV1B24qwAAPh"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import os\n",
        "from skimage.transform import resize\n",
        "from imageio import imread\n",
        "import datetime\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import abc\n",
        "from sys import getsizeof"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXnIoy-6AAPj"
      },
      "source": [
        "We set the random seed so that the results don't vary drastically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AqvzO1kXAAPk"
      },
      "outputs": [],
      "source": [
        "# Setting seed value\n",
        "np.random.seed(30)\n",
        "import random as rn\n",
        "rn.seed(30)\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6on9TJ1AAPl"
      },
      "source": [
        "## Loading Dataset\n",
        "\n",
        "Reading the folder names for training and validation data. Also set the `batch_size` here."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QP1twuOdAnTy",
        "outputId": "8ce4aae7-557f-4b7e-dfee-3f9a8eae5503"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xCmQ4vKvAAPm"
      },
      "outputs": [],
      "source": [
        "train_doc = np.random.permutation(open('/content/drive/MyDrive/Project_data/train.csv').readlines())\n",
        "val_doc = np.random.permutation(open('/content/drive/MyDrive/Project_data/val.csv').readlines())\n",
        "batch_size = 40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcV1ThlhAAPn"
      },
      "source": [
        "## Generator\n",
        "\n",
        "This is one of the most important part of the code. The overall structure of the generator has been given here. In the generator, we are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. (We can experiment with `img_idx`, `y`,`z` and normalization until we can get high accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2Y61NCy4AAPo"
      },
      "outputs": [],
      "source": [
        "# Defining for Cropping and Resizing images\n",
        "def cropAndResize(image,HEIGHT_DIMENSION,WIDTH_DIMENSION):\n",
        "    #crop the images and resize them.  \n",
        "    #Note that the images are of 2 different shape& the conv3D will throw error if the inputs in a batch have different shapes.\n",
        "    # CROPPING (making aspect ratio same)\n",
        "    if abs(image.shape[0]-image.shape[1])%2==0 and image.shape[0]!=image.shape[1]:\n",
        "        dimension_diff=abs(image.shape[0]-image.shape[1])\n",
        "        cropping_ratio=dimension_diff//2\n",
        "        if image.shape[0]>image.shape[1]:\n",
        "            image=image[cropping_ratio:image.shape[0]-cropping_ratio,:,:]\n",
        "        elif image.shape[0]<image.shape[1]:\n",
        "            image=image[:,cropping_ratio:image.shape[1]-cropping_ratio,:]\n",
        "                    \n",
        "    # RESIZING\n",
        "    if image.shape[0]>120 or image.shape[1]>120:\n",
        "        image=resize(image, (120, 120))\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n1DsomEkAAPq"
      },
      "outputs": [],
      "source": [
        "# Defininng Generator function\n",
        "def generator(source_path, folder_list, batch_size):\n",
        "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
        "    img_idx = 7,9,10,11,12,13,14,15,16,17,18,19,20,21,22,24 #create a list of image no's you want to use for a particular video\n",
        "    while True:\n",
        "        t = np.random.permutation(folder_list)\n",
        "        num_batches = len(t)//batch_size                 # calculate the number of batches\n",
        "        remaining_batch_size=len(t)%batch_size\n",
        "        for batch in range(num_batches):                 # we iterate over the number of batches\n",
        "            batch_data = np.zeros((batch_size,16,120,120,3)) # x is the no. of images you use for each video,(y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((batch_size,5))      # batch_labels is the one hot representation of the output\n",
        "            for folder in range(batch_size):             # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # Read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx):      # Iterate over the frames/images of a folder to read them in\n",
        "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                    image_resized = cropAndResize(image,120,120)                    \n",
        "                    batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255         # Normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255         # Normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255         # Normalise and feed in the image\n",
        "                    \n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels                # you yield the batch_data and the batch_labels\n",
        "        for batch in range(num_batches,num_batches+1):    # iterate over the number of batches\n",
        "            batch_data = np.zeros((remaining_batch_size*2,16,120,120,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((remaining_batch_size*2,5))       # batch_labels is the one hot representation of the output\n",
        "            for folder in range(remaining_batch_size):                # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                    image_resized = cropAndResize(image,120,120)                    \n",
        "                    batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255        #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255        #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255        #normalise and feed in the image\n",
        "                    \n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN5zBck4AAPr"
      },
      "source": [
        "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "duB78bvpAAPs"
      },
      "outputs": [],
      "source": [
        "# Creating fuction for plotting results in graph\n",
        "from matplotlib import pyplot as plt\n",
        "def plot(history):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
        "    axes[0].plot(history.history['loss'])   \n",
        "    axes[0].plot(history.history['val_loss'])\n",
        "    axes[0].legend(['loss','val_loss'])\n",
        "\n",
        "    axes[1].plot(history.history['categorical_accuracy'])   \n",
        "    axes[1].plot(history.history['val_categorical_accuracy'])\n",
        "    axes[1].legend(['categorical_accuracy','val_categorical_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNkgEzXNAAPt",
        "outputId": "bf3a3bdc-38c2-4a67-c120-04b3410a9343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sequences = 663\n",
            "Validation sequences = 100\n",
            "Epochs = 15\n"
          ]
        }
      ],
      "source": [
        "curr_dt_time = datetime.datetime.now()\n",
        "train_path = '/home/datasets/Project_data/train'\n",
        "val_path = '/home/datasets/Project_data/val'\n",
        "num_train_sequences = len(train_doc)\n",
        "print('Training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('Validation sequences =', num_val_sequences)\n",
        "num_epochs = 15              # choose the number of epochs\n",
        "print ('Epochs =', num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WUnbIBqAAPu"
      },
      "source": [
        "## Model\n",
        "\n",
        "Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Kg5IRVosAAPu"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation,Dropout,GlobalAveragePooling3D,LSTM,GlobalAveragePooling2D\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D,Conv2D,MaxPooling2D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau,EarlyStopping\n",
        "from keras import optimizers\n",
        "img_idx = 7,9,10,11,12,13,14,15,16,17,18,19,20,21,22,24\n",
        "input_shape = (len(img_idx), 120, 120, 3)\n",
        "np.random.seed(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xvZo9j0AAPv"
      },
      "source": [
        "### Model 1: Simple Conv3D\n",
        "### Batch size=15, epoch=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MtYgigrUAAPv"
      },
      "outputs": [],
      "source": [
        "#model = Sequential()\n",
        "#model.add(Conv3D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
        "#model.add(MaxPooling3D(pool_size=2))\n",
        "\n",
        "#model.add(Conv3D(64, kernel_size=3, activation='relu'))\n",
        "#model.add(MaxPooling3D(pool_size=2))\n",
        "\n",
        "#model.add(Flatten())\n",
        "#model.add(Dense(256, activation='relu'))\n",
        "#model.add(Dense(5, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apCFn4NmAAPw"
      },
      "source": [
        "1. After the model creation, the next step is to `compile` the model. In the `summary` of the model, we'll see the total number of parameters to train.\n",
        "2. Create the `train_generator` and the `val_generator` which will be used in `.fit_generator`.\n",
        "3. The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make.\n",
        "\n",
        "These steps are same for all the model. These code is given in final model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_joHaloAAPw"
      },
      "source": [
        "Now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cQa1YT3fAAPx"
      },
      "outputs": [],
      "source": [
        "#model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        " #                   callbacks=callbacks_list, validation_data=val_generator, \n",
        "  #                  validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCMiw-kFAAPx"
      },
      "source": [
        "### Model 2: Conv3D with BatchNormalization\n",
        "### Batch size=40,epoch=15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MqK3t5CRAAPx"
      },
      "outputs": [],
      "source": [
        "# model = Sequential()\n",
        "\n",
        "# model.add(Conv3D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
        "# model.add(Conv3D(64, kernel_size=3, activation='relu'))\n",
        "# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "# model.add(Conv3D(128, kernel_size=3, activation='relu'))\n",
        "# model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "# model.add(Conv3D(256, kernel_size=(1, 3, 3), activation='relu'))\n",
        "# model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv3D(512, kernel_size=(1, 3, 3), activation='relu'))\n",
        "# model.add(Conv3D(512, kernel_size=(1, 3, 3), activation='relu'))\n",
        "# model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(512, activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dense(5, activation='softmax'))\n",
        "# param=16,706,309,Trainable params: 16,703,365,Non-trainable params: 2,944"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lMGy3jPZAAPy"
      },
      "outputs": [],
      "source": [
        "#model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "  #                  callbacks=callbacks_list, validation_data=val_generator, \n",
        "  #                  validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COsmXUPqAAPz"
      },
      "source": [
        "### Model 3: Conv3D with BatchNormalization,Dropout\n",
        "### batch_size=30,epoch=20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hvG8vHYuAAPz"
      },
      "outputs": [],
      "source": [
        "#model = Sequential()\n",
        "\n",
        "#model.add(Conv3D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
        "#model.add(Conv3D(64, kernel_size=3, activation='relu'))\n",
        "#model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "#model.add(Conv3D(128, kernel_size=3, activation='relu'))\n",
        "#model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "#model.add(Conv3D(256, kernel_size=(1, 3, 3), activation='relu'))\n",
        "#model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "#model.add(Conv3D(512, kernel_size=(1, 3, 3), activation='relu'))\n",
        "#model.add(Conv3D(512, kernel_size=(1, 3, 3), activation='relu'))\n",
        "#model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "#model.add(Flatten())\n",
        "#model.add(Dense(512, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dense(5, activation='softmax'))\n",
        "#Total params: 16,706,309, Trainable params: 16,703,365, Non-trainable params: 2,944"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MQdCRAegAAP0"
      },
      "outputs": [],
      "source": [
        "#model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        " #                   callbacks=callbacks_list, validation_data=val_generator, \n",
        "  #                  validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maTdnlhFAAP0"
      },
      "source": [
        "### Model 4: Conv3D with BatchNormalization,Dropout,GlobalAveragePooling\n",
        "### batch_size=40,epoch=30      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ispPOztBAAP0"
      },
      "outputs": [],
      "source": [
        "# model = Sequential()\n",
        "\n",
        "# model.add(Conv3D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
        "# model.add(Conv3D(64, kernel_size=3, activation='relu'))\n",
        "# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Conv3D(128, kernel_size=3, activation='relu'))\n",
        "# model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Conv3D(256, kernel_size=(1, 3, 3), activation='relu'))\n",
        "# model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(GlobalAveragePooling3D())\n",
        "# model.add(Dense(512, activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dense(5, activation='softmax'))'''\n",
        "\n",
        "# Total params: 712,453, Trainable params: 710,533, Non-trainable params: 1,920"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "X3TVEnLiAAP1"
      },
      "outputs": [],
      "source": [
        "#history=model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        " #                   callbacks=callbacks_list, validation_data=val_generator, \n",
        "  #                  validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bckqI6LzAAP1"
      },
      "outputs": [],
      "source": [
        "# plot(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcoKkxjXAAP2"
      },
      "source": [
        "### Model 5: Conv 2D + LSTM\n",
        "### Batch_size=32,epoch=40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dVY_B7KiAAP2"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(TimeDistributed(Conv2D(32, (3,3), activation='relu'), input_shape=input_shape))\n",
        "model.add(TimeDistributed(MaxPooling2D((2,2))))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(TimeDistributed(Conv2D(64, (3,3), activation='relu')))\n",
        "model.add(TimeDistributed(MaxPooling2D((2,2))))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(TimeDistributed(GlobalAveragePooling2D()))\n",
        "model.add(TimeDistributed(Dense(64, activation='relu')))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(128))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "#Total params: 124,165, Trainable params: 123,589, Non-trainable params: 576"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "B_mKuNNLAAP2"
      },
      "outputs": [],
      "source": [
        " #history=model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "  #                  callbacks=callbacks_list, validation_data=val_generator, \n",
        "   #                 validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vyxBpsGMAAP3"
      },
      "outputs": [],
      "source": [
        "#plot(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgzywbamAAP3"
      },
      "source": [
        "### Model 6: Conv2D + GRU\n",
        "### Batch_size=32,epoch=40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "k_SCW-lKAAP4"
      },
      "outputs": [],
      "source": [
        "# model = Sequential()\n",
        "# model.add(TimeDistributed(Conv2D(32, (3,3), activation='relu'), input_shape=input_shape))\n",
        "# model.add(TimeDistributed(MaxPooling2D((2,2))))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(TimeDistributed(Conv2D(64, (3,3), activation='relu')))\n",
        "# model.add(TimeDistributed(MaxPooling2D((2,2))))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(TimeDistributed(GlobalAveragePooling2D()))\n",
        "# model.add(TimeDistributed(Dense(64, activation='relu')))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(GRU(128))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dense(5, activation='softmax'))\n",
        "# Total params: 99,845, Trainable params: 99,269, Non-trainable params: 576"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "hlWZ-xVcAAP4"
      },
      "outputs": [],
      "source": [
        "# history=model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "  #                  callbacks=callbacks_list, validation_data=val_generator, \n",
        "   #                 validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XAR5LVItAAP5"
      },
      "outputs": [],
      "source": [
        "# plot(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMVyR1_pAAP5"
      },
      "source": [
        "### Model 7: TransferLearning (VGG16) + LSTM\n",
        "### Batch_size=32, epoch=30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2ZApMheVAAP5"
      },
      "outputs": [],
      "source": [
        "# from keras.applications.vgg16 import VGG16\n",
        "# VGG16_transfer = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=False)\n",
        "# model = Sequential()\n",
        "# model.add(TimeDistributed(VGG16_transfer,input_shape=input_shape))\n",
        "# for layer in model.layers:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# model.add(TimeDistributed(BatchNormalization()))\n",
        "# model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "# model.add(TimeDistributed(Flatten()))\n",
        "# model.add(LSTM(128))\n",
        "# model.add(Dropout(0.25))\n",
        "# model.add(Dense(64,activation='relu'))\n",
        "# model.add(Dropout(0.25))\n",
        "        \n",
        "# model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "# Total params: 15,053,509, Trainable params: 337,797, Non-trainable params: 14,715,712,  image_size- 120,120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6jaVtPr9AAP6"
      },
      "outputs": [],
      "source": [
        "#history=model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        " #                   callbacks=callbacks_list, validation_data=val_generator, \n",
        "  #                  validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDuRt7XAAAP6"
      },
      "source": [
        "### Model 8: VGG16 + LSTM with BatchNormalization\n",
        "### batch_size=40,epoch=30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9zBkS93kAAP7"
      },
      "outputs": [],
      "source": [
        "# from keras.applications.vgg16 import VGG16\n",
        "# VGG16_transfer = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=False)\n",
        "# model = Sequential()\n",
        "# model.add(TimeDistributed(VGG16_transfer,input_shape=input_shape))\n",
        "# for layer in model.layers:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# model.add(TimeDistributed(BatchNormalization()))\n",
        "# model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "# model.add(TimeDistributed(Flatten()))\n",
        "# model.add(LSTM(128))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dense(64,activation='relu'))\n",
        "# model.add(Dropout(0.25))\n",
        "        \n",
        "# model.add(Dense(5, activation='softmax'))\n",
        "# loss: 0.1665 - categorical_accuracy: 0.9402 - val_loss: 2.2150 - val_categorical_accuracy: 0.4083 - lr: 0.0010"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "mBmohxaaAAP7"
      },
      "outputs": [],
      "source": [
        "# history=model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "#                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "#                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "S6NiQhblAAP8"
      },
      "outputs": [],
      "source": [
        "# plot(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn9Ry5xIAAP8"
      },
      "source": [
        "### Model 9: VGG16 + GRU\n",
        "### batch_size=40,epoch=20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6XNfPIOzAAP8"
      },
      "outputs": [],
      "source": [
        "# from keras.applications.vgg16 import VGG16\n",
        "# VGG16_transfer = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=False)\n",
        "# model = Sequential()\n",
        "# model.add(TimeDistributed(VGG16_transfer,input_shape=input_shape))\n",
        "# for layer in model.layers:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# model.add(TimeDistributed(BatchNormalization()))\n",
        "# model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "# model.add(TimeDistributed(Flatten()))\n",
        "# model.add(GRU(128))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(64,activation='relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "        \n",
        "# model.add(Dense(5, activation='softmax'))\n",
        "# Total params: 14,972,357, Trainable params: 256,389, Non-trainable params: 14,715,968"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "z5iRPsKoAAP9"
      },
      "outputs": [],
      "source": [
        "#history=model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        " #                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "  #                   validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "aPlSjDDYAAP9"
      },
      "outputs": [],
      "source": [
        "# plot(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGIS5BWKAAP9"
      },
      "source": [
        "## Final Model\n",
        "### Model 10: TransferLearning(Mobilenet) + GRU\n",
        "### batch size = 40, epoch=15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K26tmkGuAAP-",
        "outputId": "1c14b740-e5f8-410e-80da-8e67afd81a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        }
      ],
      "source": [
        "from keras.applications import mobilenet\n",
        "\n",
        "mobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(TimeDistributed(mobilenet_transfer,input_shape=input_shape))\n",
        " \n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "model.add(GRU(128))\n",
        "model.add(Dropout(0.25))\n",
        "        \n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "        \n",
        "model.add(Dense(5, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWM2HEhlAAP-"
      },
      "source": [
        "### Compile \n",
        "After the model creation, the next step is to `compile` the model. In`summary` of the model, we'll see the total number of parameters to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq0Rs5vhAAP-",
        "outputId": "fd2c8807-eb73-46eb-daf8-b366c04d4d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " time_distributed_6 (TimeDis  (None, 16, 3, 3, 1024)   3228864   \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_7 (TimeDis  (None, 16, 3, 3, 1024)   4096      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_8 (TimeDis  (None, 16, 1, 1, 1024)   0         \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_9 (TimeDis  (None, 16, 1024)         0         \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 128)               443136    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,693,253\n",
            "Trainable params: 3,669,317\n",
            "Non-trainable params: 23,936\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "optimiser =tf.keras.optimizers.Adam() #write your optimizer\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "OCSx1FzdAAP-"
      },
      "outputs": [],
      "source": [
        "# create the `train_generator` and the `val_generator` which is used in `.fit_generator`.\n",
        "\n",
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "YybR_MSlAAP_"
      },
      "outputs": [],
      "source": [
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
        "\n",
        "LR =ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4) # write the REducelronplateau code here\n",
        "earlystop = EarlyStopping( monitor=\"val_loss\", min_delta=0,patience=10,verbose=1)\n",
        "callbacks_list = [checkpoint, LR, earlystop]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukoHH_CwAAP_"
      },
      "source": [
        "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "NDR4SI3GAAP_"
      },
      "outputs": [],
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcKIHZiPAAQA"
      },
      "source": [
        "### Fit Model\n",
        "\n",
        "Now fit the model. This will start training the model and with the help of the checkpoints, we'll be able to save the model at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "SiUAAV1NAAQA",
        "outputId": "a37e6687-6145-4bbc-f811-bc02155e3064"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source path =  /home/datasets/Project_data/train ; batch size = 40\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-9b8f32d37e0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history=model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n\u001b[1;32m      2\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-ef9fea5e8a99>\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m(source_path, folder_list, batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# batch_labels is the one hot representation of the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m             \u001b[0;31m# iterate over the batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Read all the images in the folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m      \u001b[0;31m# Iterate over the frames/images of a folder to read them in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/datasets/Project_data/train/WIN_20180926_17_33_46_Pro_Left_Swipe_new'"
          ]
        }
      ],
      "source": [
        "history=model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSHw6cyqAAQA"
      },
      "source": [
        "### Plot graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj0aqS7iAAQA"
      },
      "outputs": [],
      "source": [
        "plot(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3Xy7Pj0AAQB"
      },
      "source": [
        "### Conclusion:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_yJBFieAAQB"
      },
      "source": [
        "By comparing above all models (Mobilenet + GRU) gives good accuracy\n",
        "\n",
        "categorical_accuracy: 0.9723\n",
        "\n",
        "val_categorical_accuracy: 0.9583"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Gesture_Recognition.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}